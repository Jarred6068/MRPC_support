---
title: 'Contribution Write-up 1/13/2021-2/26/2021'
subtitle: 'University of Idaho Department of Statistical Science'
author: "Jarred Kvamme"
date: '2/25/2021'
output:
  pdf_document: default
  html_document: default
  word_document: default
fontsize: 10pt
bibliography: biblies.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.width=12, fig.height=8)
```


\section*{Operations in January and February}

\subsection*{Re-analysis of GTEx Using {\bf MRPC-ADDIS}}

Implementation of the ADDIS improvement to the MRPC algorithm: This involved adapting the existing scripts developed by M. Badsha to rerun the GTEx data using the ADDIS version of the FDR control. The distribution of model types (M0, M1, ... M4, Other) relative to each tissue was retained from the analyses for both \textbf{ MRPC-LOND} and \textbf{ MRPC-ADDIS} for comparison. Additonally, a host of programs were developed to identify the specific classification of each trio analyzed in each tissue. These programs were further adapted to identify the number of trios for each tissue classified as cis or trans mediated (M1 type 1 or M1 type 2). Such information was again compared with results from \textbf{MRPC LOND} with the intention of understanding the differences in inferred networks between the two FDR control methods.    
    
In General, \textbf{MRPC-ADDIS} loosens the rejection threshold such that more edges/directions are inferred. There was typically a reshuffling of Model types with some specific trends such as most M1's whose graph classification was changed was converted to M2 or M4. M0's were converted to either M1, M2, M3, or M4 (with a large number in each tissue transferring to M3). The path by which M1 was converted to M2 or M4 was generally a direction flip between the cis and trans leading to a dependence structure of the cis gene dependent on both the trans gene and variant. The other method was the inference of an direct edge between the variant and both genes and a general edge between the cis and trans gene. 


```{r, fig.cap="Density plots showing the distribution of each graph type under both ADDIS and LOND", echo=FALSE, warning=FALSE, message=FALSE}
rraddis<- read.csv("C:/Users/Bruin/Desktop/Research Assistantship/ADDIS_ReRun/ReRunADDIS_Summary.csv")
rrlond<- read.csv("C:/Users/Bruin/Desktop/Research Assistantship/ADDIS_ReRun/ReRunLOND_Summary.csv")
nam=c("M0","M1","M2","M3","M4","Other")
modcts=rraddis[,c(6:11)]
colnames(modcts)=nam
modcts2=rrlond[,c(6:11)]
colnames(modcts2)=nam
modpcts=rraddis[,c(12:17)]
colnames(modpcts)=nam
modpcts2=rrlond[,c(12:17)]
colnames(modpcts2)=nam

L=rep("LOND", dim(modcts2)[1])
A=rep("ADDIS", dim(modcts)[1])

AplusL=c(A,L)

#pairs(modcts)
#pairs(modcts2)

plot.list=list()

# par(mfrow=c(2,3))
# for(i in 1:6){
#   hist(modcts[,i],main = paste("histogram of:", nam[i], sep = " "))
# }
# par(mfrow=c(1,1))




library(ggplot2)
for(i in 1:6){

  df=cbind.data.frame( FDR=as.factor(AplusL), M=c(modcts[,i], modcts2[,i]))
  
  plot.list[[i]]=ggplot(df, aes(x=M, color=FDR, fill=FDR)) +geom_density(alpha=0.3)+
    ggtitle(label = paste("Density plot for:", nam[i], sep = " "))
    
}


library(gridExtra)
grid.arrange(grobs=plot.list,nrows=3, ncol=2)
```






```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(knitr)

#change table
average.change.ct=colMeans(modcts-modcts2)
average.change.ct.sd=apply(modcts-modcts2, 2, sd)
average.change.pct=colMeans(modpcts-modpcts2)
average.change.pct.sd=apply(modpcts-modpcts2,2,sd)

change.table=rbind.data.frame(average.change.ct, average.change.ct.sd, 
                              average.change.pct, average.change.pct.sd)
colnames(change.table)=nam
row.names(change.table)=c("mean.change.ct", "SD.change.ct", "mean.change%", "SD.change%")

kable(change.table, caption = "table of the average difference in count and percentage between ADDIS and LOND and their standard errors for the differences", digits = 5)


#Bootstrap CI's

Addis.cIs=as.data.frame(matrix(0, nrow = 6, ncol = 3))
colnames(Addis.cIs)=c("lower.limit", "mean","upper.limit")
row.names(Addis.cIs)=nam
B=1000
alpha=0.05


#calculate confidence interval for each model type avg count(ADDIS)
for(i in 1:6){
  bsdata=rep(0,B)
  
  for(j in 1:B){
    
    bsdata[j]=mean(sample(modcts[,i], dim(modcts)[1], replace = TRUE))
    
  }
  
  #hist(bsdata)
  SE=sd(bsdata)
  M=mean(bsdata)
  lower=M-qnorm(alpha/2)*(SE/sqrt(length(bsdata)))
  upper=M+qnorm(alpha/2)*(SE/sqrt(length(bsdata)))
  
  Addis.cIs[i,]=round(c(lower,M,upper),3)
}



Lond.cIs=as.data.frame(matrix(0, nrow = 6, ncol = 3))
colnames(Lond.cIs)=c("lower.limit", "mean","upper.limit")
row.names(Lond.cIs)=nam
#calculate confidence interval for each model type avg count(LOND)
for(i in 1:6){
  bsdata=rep(0,B)
  
  for(j in 1:B){
    
    bsdata[j]=mean(sample(modcts2[,i], dim(modcts2)[1], replace = TRUE))
    
  }
  
  #print(bsdata)
  #hist(bsdata, breaks = 15)
  SE=sd(bsdata)
  M=mean(bsdata)
  lower=M-qnorm(alpha/2)*(SE/sqrt(length(bsdata)))
  upper=M+qnorm(alpha/2)*(SE/sqrt(length(bsdata)))
  
  Lond.cIs[i,]=round(c(lower,M,upper),3)
}


kable(Lond.cIs, caption = "Table of confidence intervals for the average count of each classified graph for all tissues under the LOND FDR control ")
kable(Addis.cIs, caption = "Table of confidence intervals for the average count of each classified graph for all tissues under the ADDIS FDR control ")

```






\subsection*{Description of Functions/Programs}

$\bullet$ 

















\newpage
\section*{Incorporation of HiC Data To Verify Trans Mediated Regulation }

$\bullet$ \textbf{2/12/2021 - 2/26/2021}   
HiC mapping quality thresholded chromatin interaction data was obtained from the ENCODE consortium for four tissues (lymphoblastoid cells, fibroblast cells, skin, and lung) to identify the presence of interaction enrichment for trios classified as trans-mediated cis regulation by \textbf{MRPC-ADDIS} [@davis2018encyclopedia; @encode2012integrated]. The binned interactions between genomic regions was extracted at 10,000 bp resolution using the \textit{StrawR} package provided by Aiden Lab for use with the \textit{.hic} file format [@durand2016juicebox]. From this data, the total number of interactions between each trio's variant and it's associated trans gene was checked. This was done by summing all interaction counts for a 200,000 bp bin around the positions of both the variant and trans gene on their respective chromosomes. This was considered the observed number of interactions for a variant/trans-gene pair. Trios that had no observed interactions (or for which the data wasn't available) were desginated missing or "NA"  
   
To identify enrichment, we conducted 10,000 resamplings of interactions between randomly selected and uniformly probable positions throughout the trans-gene and variant's respective chromosomes. As with the observed pairs, the total number of interactions was counted for a 200,000 bp bin around the gene and variant chromosomal positions. This served as a comparative unique null distribution of counts for each trio and was used to ascertain the upper-tail probability of a pair's observed number of interactions.    
    
Many of the randomly selected 200,000 bp regions had either no interactions or no reads available. Without the ability to discern between unavailable data and 0 interaction values between two randomly selected regions, all empty interaction pairs were treated as missing data points and designated "NA". Therefore, the 10,000 resamples for each trio were partitioned into available data (non-NA) and unavailable data (NA's). In calculating the upper tail probability it is of significance to consider the quantity of unavailable data for a specific variant/trans-gene pair.

\[P_{obs}=P\big (A_i\geq A_{obs} \big | A_i\neq NA\big) = \frac{P(A_i\geq A_{obs} \ \cap A_i\neq NA)}{P(A_i \neq NA)} \]

using the following indicator functions:
\[ \text{Let} \ f(A_i) = \begin{cases} 1, & \text{if} \ A_i\neq \text{NA} \ \forall \ i \in 1:N\\ 0, & \text{else} \  \end{cases}   \]

\[ \text{Let} \ g(B_j) = \begin{cases} 1, & \text{if} \ B_j\geq B_{obs} \ \forall \ j \in 1:n_1, \ B\subseteq A\\ 0, & \text{else} \  \end{cases}   \]

Where $n_1$ is the number of resamples not in the set of $NA$'s and $n_2$ be number of resamples in the complement event such that $n_1 + n_2 = N =10,000$. Thus we have:

\[ = \frac{\frac{\sum_{j=1}^{n_1} g(B_j)}{\sum_{i=1}^N f(A_i)} \times \frac{\sum_{i=1}^N f(A_i)}{N}}{\left(\frac{\sum_{j=1}^{n_1} g(B_j)}{\sum_{i=1}^N f(A_i)} \times \frac{\sum_{i=1}^N f(A_i)}{N}\right)+\left( \frac{n_1 -\sum_{j=1}^{n_1} g(B_j)}{\sum_{i=1}^N f(A_i)} \times \frac{\sum_{i=1}^N f(A_i)}{N} \right)}\]

\[ = \frac{   \frac{\sum_{j=1}^{n_1} g(B_j)}{N}    }{ \frac{\sum_{j=1}^{n_1} f(B_j)}{N} +\frac{n_1 - \sum_{j=1}^{n_1} f(B_j)}{N}  }\]

\[ \frac{\sum_{j=1}^{n_1} g(B_j)}{\sum_{j=1}^{n_1} g(B_j)+ \left(n_1 - \sum_{j=1}^{n_1} g(B_j)\right)} = \frac{\sum_{j=1}^{n_1} g(B_j)}{n_1}  \]

Note that a much simpler approach is to notice the exclusivity of the complement event $A_i = NA$ and the event $A_i\geq A_{obs}$ which leads to the realization that $P\big (A_i\geq A_{obs} \big | A_i\neq NA\big) \equiv P\big(B_j \geq B_{obs}\big) \ \forall \ j \in 1:n_1$    
    
Significant enrichment was defined as an observed probability less than the threshold $\alpha$ taken at the usual level of $\alpha=0.05$. To control for the false detection of significant enrichment, two FWER and one FDR correction were applied to the observed probabilities which included: Holm-Bonferroni (FWER), Hochberg step-up (FWER), and the BH method (FDR).    

\textbf{Some Notes On q-values, FWER and FDR}:

$\bullet$ In the special case of all Null hypotheses being true the FWER and FDR are equivalent. In all other cases the FWER adjustments control the expected number of type I errors among all hypotheses/tests. The FDR controls the expected number of type I errors among significant tests. 

$\bullet$ FWER methods are more stringent then FDR methods because of the consideration of all tests, therefore FDR methods are more powerful. 

$\bullet$ In general, controling the FWER lowers the risk of a type I error at the expense of an increased risk of committing a type II error (failing to reject a null hypothesis). Holm-Bonferroni has a lower risk of type II error than the standard Bonferroni procedure and is therefore uniformly more powerful 

$\bullet$ Just as the p-value gives the expected False Positive Rate (FPR) by rejecting any hypotheses with a p-value at or below the FPR, the q-value similarly gives the Posititive False Discovery Rate (pFDR)/type I error by rejecting any hypothesis with a q-value at or below the pFDR

































\newpage